{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "195a23af",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "Build a model that can detect whether a video is \"real\" or \"fake\" by analyzing facial features across multiple frames using:\n",
    "\n",
    "MTCNN for face detection\n",
    "\n",
    "MobileNetV2 for feature extraction\n",
    "\n",
    "LSTM for modeling temporal (frame-by-frame) dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7143ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd2e2a",
   "metadata": {},
   "source": [
    "Unzips the FaceForensics++ dataset (which includes folders like real and fake containing video files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746a4917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['fake', 'real']\n"
     ]
    }
   ],
   "source": [
    "zip_path = r\"D:\\Spring_2025\\STT_811\\FF++.zip\"\n",
    "extract_to = \"faceforensics_data\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(\"Extracted files:\", os.listdir(extract_to))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e9dd8",
   "metadata": {},
   "source": [
    "Initialize MTCNN to detect faces in video frames.\n",
    "- IMG_SIZE = 224: The image size to which faces will be resized.\n",
    "- FRAMES_PER_VIDEO = 5: Number of frames to extract from each video.\n",
    "\n",
    "The purpose of the function \"extract_faces_from_video\" is:\n",
    "\n",
    "For each video:\n",
    "- Extracts 5 evenly spaced frames.\n",
    "- Converts to RGB and uses MTCNN to detect faces.\n",
    "- Crops and resizes the detected face to 224x224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f76c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()\n",
    "IMG_SIZE = 224\n",
    "FRAMES_PER_VIDEO = 5\n",
    "\n",
    "def extract_faces_from_video(video_path, frames_to_extract=FRAMES_PER_VIDEO):\n",
    "    faces = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    for i in np.linspace(0, total_frames - 1, frames_to_extract, dtype=int):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        detections = detector.detect_faces(frame_rgb)\n",
    "        if detections:\n",
    "            x, y, w, h = detections[0]['box']\n",
    "            x, y = max(0, x), max(0, y)\n",
    "            face = frame_rgb[y:y+h, x:x+w]\n",
    "            if face.size == 0:\n",
    "                continue\n",
    "            face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "            faces.append(face)\n",
    "    cap.release()\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cf23a",
   "metadata": {},
   "source": [
    "- Loads a pretrained MobileNetV2 model (without top layer).\n",
    "\n",
    "- Extracts a 1280-dimensional vector per face image using GlobalAveragePooling2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916551ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=GlobalAveragePooling2D()(base_model.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090aeab",
   "metadata": {},
   "source": [
    "\n",
    "Iterates through the real and fake folders.\n",
    "\n",
    "For each video:\n",
    "- Extracts 5 face frames.\n",
    "- Extracts 1280-dim feature vector per face.\n",
    "- Combines them into a (5, 1280) array representing the video.\n",
    "- Appends the array to sequences, and the label to labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 200 videos in real folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 115/200 [15:28<12:04,  8.53s/it]"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "classes = ['real', 'fake']\n",
    "\n",
    "for label, folder_name in enumerate(classes):\n",
    "    folder_path = os.path.join(extract_to, folder_name)\n",
    "    video_files = os.listdir(folder_path)\n",
    "    print(f\"Processing {len(video_files)} videos in {folder_name} folder\")\n",
    "\n",
    "    for video_file in tqdm(video_files):\n",
    "        video_path = os.path.join(folder_path, video_file)\n",
    "        faces = extract_faces_from_video(video_path)\n",
    "\n",
    "        if len(faces) < FRAMES_PER_VIDEO:\n",
    "            continue  # Skip incomplete videos\n",
    "\n",
    "        video_features = []\n",
    "        for face in faces:\n",
    "            face = preprocess_input(face.astype(np.float32))\n",
    "            features = feature_extractor.predict(np.expand_dims(face, axis=0), verbose=0)\n",
    "            video_features.append(features.squeeze())\n",
    "\n",
    "        sequences.append(np.array(video_features))  # Shape: (5, 1280)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95f33f",
   "metadata": {},
   "source": [
    "- Saves the extracted features and labels for later use to avoid reprocessing.\n",
    "- Y_seq is one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b60acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq = np.array(sequences)\n",
    "y_seq = to_categorical(labels)\n",
    "\n",
    "np.save(\"X_seq.npy\", X_seq)\n",
    "np.save(\"y_seq.npy\", y_seq)\n",
    "\n",
    "print(\"Shape of X_seq:\", X_seq.shape)\n",
    "print(\"Shape of y_seq:\", y_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c90b2f",
   "metadata": {},
   "source": [
    "- Splits the dataset (80% train, 20% test) while keeping label distribution balanced (stratify=y_seq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b454b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, stratify=y_seq, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b457a4e1",
   "metadata": {},
   "source": [
    "Builds a sequential model to learn temporal patterns across face features in video:\n",
    "\n",
    "- LSTM(64) learns the sequence over 5 frames.\n",
    "- Dropout to prevent overfitting.\n",
    "- Dense(2, softmax) outputs class probabilities (real/fake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4601478",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(FRAMES_PER_VIDEO, 1280), return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f772a",
   "metadata": {},
   "source": [
    "Trains the model over 10 epochs using categorical_crossentropy loss.\n",
    "Uses adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b92abf",
   "metadata": {},
   "source": [
    "- Converts predicted probabilities to class labels.\n",
    "- Prints a classification report (precision, recall, F1-score).\n",
    "- Plots a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred_classes), annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
